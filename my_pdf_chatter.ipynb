{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "include sources by paragraph:  \n",
    "  https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa  \n",
    "  first split by paragraph   \n",
    "  assign unique paragraph key  \n",
    "  pass and prompt to gpt  \n",
    "\n",
    "implement streaming:  \n",
    "  https://python.langchain.com/docs/modules/model_io/models/llms/streaming_llm  \n",
    "\n",
    "improve gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Langchain tools\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Gradio\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key\n",
    "load_dotenv()\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# PDF path\n",
    "pdf_path = \"./pdf/Dupre_economy_as_science.pdf\"\n",
    "\n",
    "# Setup LLM\n",
    "llm = OpenAI(temperature=0, top_p=0.1, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./pdf/Dupre_economy_as_science.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load_and_split()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "qdrant = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"Dupre\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup system prompt\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question \n",
    "at the end. If you don't know the answer, just say that you don't know, \n",
    "don't try to make up an answer. Makes sure to answer the question thoroughly.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give some user prompt examples\n",
    "\n",
    "example_questions=[\n",
    "    \"What is the main argument of this paper?\",\n",
    "    \"How does the author support the main argument?\",\n",
    "    \"What is the author's conclusion?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define QA function\n",
    "# This works\n",
    "\n",
    "def get_answers(query, history):\n",
    "    found_docs = qdrant.max_marginal_relevance_search(query, k=2, fetch_k=10)\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "    summary = chain.run({'question': query, 'input_documents': found_docs})\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define QA function\n",
    "# experimental\n",
    "\n",
    "def get_answers_exp(query, history):\n",
    "    found_docs = qdrant.max_marginal_relevance_search(query, k=2, fetch_k=10)\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "    summary = chain.run(question = query, input_documents = found_docs)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Gradio\n",
    "\n",
    "pdf_chatter = gr.ChatInterface(fn=get_answers_exp, examples=example_questions, title=\"PDF Chatter\")\n",
    "pdf_chatter.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfchatenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
