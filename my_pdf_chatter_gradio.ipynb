{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###To DO\n",
    "include sources by paragraph:\n",
    "  https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa\n",
    "  first split by paragraph\n",
    "  assign unique paragraph key\n",
    "  pass and prompt to gpt\n",
    "\n",
    "implement streaming:\n",
    "  https://python.langchain.com/docs/modules/model_io/models/llms/streaming_llm\n",
    "\n",
    "implement gradio:\n",
    "  https://www.gradio.app/guides/creating-a-chatbot-fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from qdrant_client import QdrantClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Set PDF path\n",
    "pdf_path = \"./dupre.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple method - Split by pages\n",
    "\n",
    "# Load and split\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Qdrant vector database\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "qdrant = Qdrant.from_documents(\n",
    "    pages,\n",
    "    embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"dupre\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if embeddings MMR search works\n",
    "\n",
    "query = \"What did dupre say in this paper?\"\n",
    "found_docs = qdrant.max_marginal_relevance_search(query, k=2, fetch_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QA chain to integrate similarity search with user queries (answer query from knowledge base)\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question \n",
    "at the end. If you don't know the answer, just say that you don't know, \n",
    "don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer in Swedish:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "query = \"what did you learn reading this paper?\"\n",
    "\n",
    "\n",
    "chain = load_qa_chain(OpenAI(temperature=0, top_p=0.1, n=1), chain_type=\"stuff\", prompt=PROMPT)\n",
    "response = chain({\"input_documents\": found_docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type()response['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector database\n",
    "db = FAISS.from_documents(pages, embeddings)\n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0, top_p=0.1, n=1), db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Transformers chatbot! Type 'exit' to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7429/3589111581.py:20: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  input_box.on_submit(on_submit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa31cef5cb1945918d4cf1d14a4201bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Please enter your question:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "def on_submit(_):\n",
    "    query = input_box.value\n",
    "    input_box.value = \"\"\n",
    "\n",
    "    if query.lower() == 'exit':\n",
    "        print(\"Thank you for using the State of the Union chatbot!\")\n",
    "        return\n",
    "\n",
    "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "    chat_history.append((query, result['answer']))\n",
    "\n",
    "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
    "    display(widgets.HTML(f'<b><font color=\"blue\">Chatbot:</font></b> {result[\"answer\"]}'))\n",
    "\n",
    "print(\"Welcome to the Transformers chatbot! Type 'exit' to stop.\")\n",
    "\n",
    "input_box = widgets.Text(placeholder='Please enter your question:')\n",
    "input_box.on_submit(on_submit)\n",
    "\n",
    "display(input_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mo/.pyenv/versions/3.11.3/envs/pdfchatenv/lib/python3.11/site-packages/gradio/routes.py\", line 538, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mo/.pyenv/versions/3.11.3/envs/pdfchatenv/lib/python3.11/site-packages/gradio/route_utils.py\", line 217, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mo/.pyenv/versions/3.11.3/envs/pdfchatenv/lib/python3.11/site-packages/gradio/blocks.py\", line 1553, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mo/.pyenv/versions/3.11.3/envs/pdfchatenv/lib/python3.11/site-packages/gradio/blocks.py\", line 1189, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mo/.pyenv/versions/3.11.3/envs/pdfchatenv/lib/python3.11/site-packages/gradio/utils.py\", line 634, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mo/.pyenv/versions/3.11.3/envs/pdfchatenv/lib/python3.11/site-packages/gradio/chat_interface.py\", line 403, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mo/.pyenv/versions/3.11.3/envs/pdfchatenv/lib/python3.11/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mo/.pyenv/versions/3.11.3/envs/pdfchatenv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/mo/.pyenv/versions/3.11.3/envs/pdfchatenv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'str' object is not callable\n"
     ]
    }
   ],
   "source": [
    "# Make Gui with Gradio\n",
    "\n",
    "history = []\n",
    "\n",
    "def response(message, history, found_docs):\n",
    "    response = qa({\"question\": message, \"chat_history\": history})\n",
    "    history.append(message)\n",
    "    history.append(response['answer'])\n",
    "    return response['answer']\n",
    "\n",
    "chat = gr.ChatInterface(response(query, history, found_docs))\n",
    "\n",
    "chat.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example from Gradio\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]  # Replace with your key\n",
    "\n",
    "def predict(message, history, found_docs):\n",
    "    history_openai_format = []\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages= history_openai_format,\n",
    "        temperature=1.0,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    partial_message = \"\"\n",
    "    for chunk in response:\n",
    "        if len(chunk['choices'][0]['delta']) != 0:\n",
    "            partial_message = partial_message + chunk['choices'][0]['delta']['content']\n",
    "            yield partial_message\n",
    "\n",
    "gr.ChatInterface(predict).queue().launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
